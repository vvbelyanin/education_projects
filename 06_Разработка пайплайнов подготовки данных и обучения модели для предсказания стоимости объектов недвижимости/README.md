Тема: Автоматизация процессов машинного обучения  
Название: Разработка пайплайнов подготовки данных и обучения модели для предсказания стоимости объектов недвижимости  
Описание: Заказчик является компанией-маркетплейсом для аренды и покупки жилой и коммерческой недвижимости, играющим роль посредника между арендодателями/продавцами и потенциальными арендаторами/покупателями. Необходимо разработать MVP алгоритма на основе машинного обучения для оценки рыночной стоимости квартиры по её характеристикам.

Этапы решения задачи    
| # | Описание | Инструменты/Ключевые слова |
|:----:|:---------------------------|:-----------------------------------------------------------|
| 1 | Сбор данных | `PostgreSQL` `pandas` `Airflow`|
| 2 | Очистка данных | `pandas` `matplotlib` `numpy` |
| 3 | Обучение модели | `DVC` `SQLAlchemy` `sklearn` `CatBoostRegressor` `S3`|


# Проект. Разработка пайплайнов подготовки данных и обучения модели

**Описание задачи**    
Яндекс Недвижимость — маркетплейс для аренды и покупки жилой и коммерческой недвижимости. Компания играет роль посредника между арендодателями/продавцами и потенциальными арендаторами/покупателями. 

**Проблема**    
Продакт-менеджеры выдвинули гипотезу, что среднемесячное количество сделок увеличится, если у сторон будет возможность провести объективную внешнюю оценку стоимости квартиры. 

**Бизнес-задача**    
Разработка MVP алгоритма на основе машинного обучения для оценки рыночной стоимости квартиры по её характеристикам.

**Задача машинного обучения**
- Тип задачи: регрессия
- Целевая переменная: цена на объект недвижимости
- Единица предсказания: уникальная квартира

**План проекта**
- Этап 1. Сбор данных
    - Создание процедуры DAG `prepare_flat_dataset` для сбора данных из таблиц `flats` и `buildings` в объединенную таблицу `combined_flats`
    - Создание функций сбора данных - `create_table`, `extract`, `transform` и `load`, а также плагина `messages` для отправки оповещений в Telegram
    - Сохранение таблицы в персональной БД
    - Результаты этапа:
        - тетрадь Jupyter Notebook [project-sprint-1-v001.ipynb](./part1_airlfow/project-sprint-1-v001.ipynb) с комментариями и выводами
        - код DAG [project-sprint-1-v001.py](./part1_airlfow/project-sprint-1-v001.py) в репозитории GitHub 
        - код плагинов [messages.py](./part1_airlfow/steps/messages.py) в репозитории GitHub 
        - таблица `combined_flats` с собранным датасетом в БД
- Этап 2. Очистка данных
    - Анализ данные на наличие дубликатов, пропусков и выбросов
    - Создание функций для обработки данных на основе проведенного анализа
    - Сохранение датасета в персональной БД
    - Результаты этапа:
        - дополненная тетрадь Jupyter Notebook [project-sprint-1-v001.ipynb](./part1_airlfow/project-sprint-1-v001.ipynb) с комментариями и выводами
        - код DAG [project-sprint-1-v002.py](./part1_airlfow/project-sprint-1-v002.py) в репозитории GitHub 
        - код плагинов [messages.py](./part1_airlfow/steps/messages.py) в репозитории GitHub 
        - таблица `clean_flats` с очищенными данными в БД
- Этап 3. Обучение модели
    - Предобработка данных для обучения
    - Создание кода обучения, разбитого на шаги, в файлах `dvc.yaml`, `params.yaml`
    - Выбор и обучение базовой модели в DVC-пайплайне 
    - Сохранение обученной модели в облачном хранилище S3.
    - Результаты этапа: 
        - тетрадь Jupyter Notebook [project-sprint-1-v003.ipynb](./part2_dvc/notebooks/project-sprint-1-v003.ipynb) с комментариями и выводами
        - файлы DVC:   
            - [dvc.yaml](./part2_dvc/dvc.yaml), 
            - [params.yaml](./part2_dvc/params.yaml), 
            - [dvc.lock](./part2_dvc/dvc.lock)    
            с кодом DVC-пайплайна в репозитории GitHub
        - скрипты:  
            - [data.py](./part2_dvc/scripts/data.py)
            - [fit.py](./part2_dvc/scripts/fit.py)
            - [evaluate.py](./part2_dvc/scripts/evaluate.py)    
             с кодом DVC-пайплайна в репозитории GitHub,
        - обученная модель, сохранённая в хранилище S3